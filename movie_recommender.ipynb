from glob import glob
from collections import defaultdict

def _find_first_csv(data_dir: str, name_patterns: List[str]) -> Optional[str]:
    if not data_dir or not os.path.isdir(data_dir):
        return None
    all_csvs = glob(os.path.join(data_dir, "**", "*.csv"), recursive=True)
    candidates = []
    for path in all_csvs:
        lower = os.path.basename(path).lower()
        for pat in name_patterns:
            if pat in lower:
                candidates.append(path)
                break
    if candidates:
        # Prefer top-level and shorter names
        candidates.sort(key=lambda p: (p.count(os.sep), len(os.path.basename(p))))
        return candidates[0]
    return None

def load_ratings_df(data_dir: str) -> pd.DataFrame:
    """Load ratings with auto-detected column names.
    Expected columns similar to MovieLens: userId, movieId, rating, [timestamp].
    Returns a DataFrame with standardized columns: user_id, movie_id, rating, timestamp(optional).
    """
    path = _find_first_csv(data_dir, ["rating", "ratings"]) if data_dir else None
    if path is None:
        raise FileNotFoundError("Could not find ratings CSV. Ensure a file like 'ratings.csv' exists.")
    df = pd.read_csv(path)
    # Normalize column names
    colmap = {c.lower(): c for c in df.columns}
    user_col = next((colmap[k] for k in colmap if k in ["userid", "user_id", "user"]), None)
    item_col = next((colmap[k] for k in colmap if k in ["movieid", "movie_id", "movie", "itemid", "item_id", "item"]), None)
    rating_col = next((colmap[k] for k in colmap if k in ["rating", "score", "rank", "stars"]), None)
    ts_col = next((colmap[k] for k in colmap if k in ["timestamp", "ts", "time"]), None)
    if not (user_col and item_col and rating_col):
        raise ValueError(f"Ratings file at {path} is missing required columns. Found: {list(df.columns)}")
    df = df.rename(columns={user_col: "user_id", item_col: "movie_id", rating_col: "rating"})
    if ts_col:
        df = df.rename(columns={ts_col: "timestamp"})
    # Cast types and clean
    df = df.dropna(subset=["user_id", "movie_id", "rating"]).copy()
    # Handle non-numeric IDs by factorizing
    if not np.issubdtype(df["user_id"].dtype, np.number):
        df["user_id"], _ = pd.factorize(df["user_id"])  # start at 0
        df["user_id"] = df["user_id"].astype(int)
    if not np.issubdtype(df["movie_id"].dtype, np.number):
        df["movie_id"], _ = pd.factorize(df["movie_id"])  # start at 0
        df["movie_id"] = df["movie_id"].astype(int)
    df["rating"] = pd.to_numeric(df["rating"], errors="coerce")
    df = df.dropna(subset=["rating"])  # remove invalid ratings
    # Clip ratings to common range if outliers exist
    rating_min, rating_max = df["rating"].min(), df["rating"].max()
    if rating_min < 0 or rating_max > 10:
        df["rating"] = df["rating"].clip(lower=0, upper=10)
    # If duplicates exist (same user/movie), average them
    if df.duplicated(subset=["user_id", "movie_id"]).any():
        df = df.groupby(["user_id", "movie_id"], as_index=False).agg({"rating": "mean", "timestamp": "max" if "timestamp" in df.columns else "first"})
    return df

def load_movies_df(data_dir: str) -> pd.DataFrame:
    """Load movies metadata with auto-detected column names.
    Expected columns similar to MovieLens: movieId, title, genres, [overview/keywords].
    Returns standardized columns: movie_id, title, genres, overview, keywords.
    """
    path = _find_first_csv(data_dir, ["movie", "movies", "title", "titles"])
    if path is None:
        # Some datasets provide metadata under "metadata" or "tmdb"
        path = _find_first_csv(data_dir, ["metadata", "tmdb", "films", "items"])
    if path is None:
        raise FileNotFoundError("Could not find movies metadata CSV. Ensure a file like 'movies.csv' exists.")
    df = pd.read_csv(path)
    colmap = {c.lower(): c for c in df.columns}
    id_col = next((colmap[k] for k in colmap if k in ["movieid", "movie_id", "movie", "itemid", "item_id", "id"]), None)
    title_col = next((colmap[k] for k in colmap if k in ["title", "name"]), None)
    genres_col = next((colmap[k] for k in colmap if k in ["genres", "genre", "listed_in"]), None)  # listed_in for Netflix
    overview_col = next((colmap[k] for k in colmap if k in ["overview", "description", "plot", "summary"]), None)
    keywords_col = next((colmap[k] for k in colmap if k in ["keywords", "tags"]), None)

    if id_col is None:
        # If there's no explicit ID, create one after factorization by title
        if title_col is None:
            raise ValueError(f"Movies metadata at {path} lacks identifiable id or title columns: {list(df.columns)}")
        df["movie_id"], _ = pd.factorize(df[title_col])
    else:
        df = df.rename(columns={id_col: "movie_id"})
    if title_col:
        df = df.rename(columns={title_col: "title"})
    else:
        df["title"] = df["movie_id"].astype(str)
    if genres_col:
        df = df.rename(columns={genres_col: "genres"})
    else:
        df["genres"] = None
    if overview_col:
        df = df.rename(columns={overview_col: "overview"})
    else:
        df["overview"] = None
    if keywords_col:
        df = df.rename(columns={keywords_col: "keywords"})
    else:
        df["keywords"] = None

    # Ensure correct dtypes
    if not np.issubdtype(df["movie_id"].dtype, np.number):
        df["movie_id"], _ = pd.factorize(df["movie_id"])  # start at 0
        df["movie_id"] = df["movie_id"].astype(int)

    # Normalize genres separators and text
    def _norm_text(x):
        if pd.isna(x):
            return ""
        if isinstance(x, str):
            return x.replace("|", ",").replace("/", ",").replace(";", ",").lower()
        return str(x)

    df["genres"] = df["genres"].apply(_norm_text)
    df["overview"] = df["overview"].apply(_norm_text)
    df["keywords"] = df["keywords"].apply(_norm_text)

    # Drop duplicates by movie_id keeping the first
    df = df.drop_duplicates(subset=["movie_id"]).copy()
    return df[["movie_id", "title", "genres", "overview", "keywords"]]


def summarize_dataset(ratings: pd.DataFrame, movies: pd.DataFrame) -> pd.DataFrame:
    num_users = ratings["user_id"].nunique()
    num_movies_rated = ratings["movie_id"].nunique()
    num_movies_meta = movies["movie_id"].nunique()
    num_ratings = len(ratings)
    sparsity = 1.0 - (num_ratings / (num_users * max(1, num_movies_rated)))
    rating_min, rating_max = ratings["rating"].min(), ratings["rating"].max()
    print(f"Users: {num_users:,} | Movies (rated): {num_movies_rated:,} | Movies (meta): {num_movies_meta:,} | Ratings: {num_ratings:,}")
    print(f"Rating scale observed: [{rating_min}, {rating_max}] | Sparsity: {sparsity:.4f}")
    return pd.DataFrame({
        "num_users": [num_users],
        "num_movies_rated": [num_movies_rated],
        "num_movies_meta": [num_movies_meta],
        "num_ratings": [num_ratings],
        "sparsity": [sparsity],
        "rating_min": [rating_min],
        "rating_max": [rating_max],
    })

# Execute loading if DATA_DIR is set
if DATA_DIR:
    try:
        ratings_df = load_ratings_df(DATA_DIR)
        movies_df = load_movies_df(DATA_DIR)
        summary_df = summarize_dataset(ratings_df, movies_df)
        display(summary_df)
    except Exception as e:
        print("Loading error:", e)
        ratings_df, movies_df = None, None
else:
    print("Set DATA_DIR to the folder with your CSV files to proceed.")import os
import json
import math
import random
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.sparse import csr_matrix
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity, linear_kernel
from sklearn.model_selection import train_test_split

# Surprise (for CF)
from surprise import Dataset as SurpriseDataset
from surprise import Reader
from surprise import SVD, KNNBasic
from surprise.model_selection import train_test_split as surprise_train_test_split
from surprise import accuracy

# Reproducibility
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

# Config: set this to your dataset directory (e.g., Kaggle download path)
DATA_DIR = os.environ.get("MOVIE_DATA_DIR", "")  # or set explicitly, e.g., "/workspace/data"

# Display
pd.set_option('display.max_colwidth', 200)
sns.set(style="whitegrid", context="notebook")

print("DATA_DIR:", DATA_DIR if DATA_DIR else "<not set>")# Install dependencies (safe to re-run)
# If running offline, comment this cell out if installs fail.
import sys, subprocess

def pip_install(packages):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q"] + packages)
    except Exception as e:
        print("Warning: pip install failed for", packages, "->", e)

required = [
    "pandas>=2.0.0",
    "numpy>=1.24.0",
    "scikit-learn>=1.3.0",
    "scipy>=1.10.0",
    "seaborn>=0.12.0",
    "matplotlib>=3.7.0",
    "surprise>=0.1.1"
]

pip_install(required)
print("Dependencies checked.")## Movie Recommendation System (CF + CBF + Hybrid)

This notebook builds a complete recommendation system using your Kaggle dataset of movie ratings and metadata. It includes:
- Data loading and cleaning (auto-detects common schemas)
- EDA: rating distributions, most rated movies, top users, genre popularity
- Collaborative Filtering (SVD) with RMSE evaluation
- Content-Based Filtering (TF-IDF over genres/keywords) with basic evaluation
- Optional Hybrid recommender combining CF and CBF
- A `recommend_movies(user_id, n=10, strategy=...)` function with examples
- Utility to update the model with new ratings

Instructions:
- Set `DATA_DIR` below to the folder containing your CSVs (e.g., the path returned by Kaggle download).
- The loader will search for files like `ratings.csv`, `movies.csv`, and metadata files with columns similar to MovieLens schemas.
- If your dataset uses different column names, the loader attempts to auto-detect them.

Optional Kaggle download example (replace with your dataset):
```python
import kagglehub
path = kagglehub.dataset_download("shivamb/netflix-shows")
print("Path to dataset files:", path)
# Then set DATA_DIR = path
```
